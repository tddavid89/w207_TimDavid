{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and test a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the iris data. In case you don't feel familiar with the iris varieties yet, here are some pictures. The petals are smaller and stick out above the larger, flatter sepals. In many flowers, the sepal is a greenish support below the petals, but the iris sepals are designed specifically as landing pads for bumblebees, and the bright yellow coloring on the sepal directs the bees down into the tight space where pollination happens.\n",
    "\n",
    "<img src=\"../Extra/iris.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris target names: ['setosa' 'versicolor' 'virginica']\n",
      "Iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print 'Iris target names:', iris.target_names\n",
    "print 'Iris feature names:', iris.feature_names\n",
    "X, Y = iris.data, iris.target\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "# Split into train and test.\n",
    "train_data, train_labels = X[:100], Y[:100]\n",
    "test_data, test_labels = X[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris feature values are real valued -- measurements in centimeters. Let's look at histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAADSCAYAAADQbYiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMzaxFX0szMzMzMzFrk/wO8KmNF5AxQ0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3317e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Iterate over the features, creating a subplot with a histogram for each one.\n",
    "for feature in range(train_data.shape[1]):\n",
    "    plt.subplot(1, 4, feature+1)\n",
    "    plt.hist(train_data[:,feature], 20)\n",
    "    plt.title(iris.feature_names[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simple, let's binarize these feature values. That is, we'll treat each measurement as either \"short\" or \"long\". I'm just going to choose a threshold for each feature.  Binning usually depends on the distribution.  There are two ways to bin data: constant bin width and constant bin size. For now let's just go with the two.\n",
    "\n",
    "This is equivalent to running the previous block of code with only two bins for plt.hist, except that we'll use a different binning approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAADSCAYAAAA7WjOOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xm0LWV95//3h0lmEM3vgoqgSYhDYlBbYhyvBowaJfZv\nJba2iVdDG01rYsfEJZr82kuMLW1i1Gim1qBX4kRMZGHHGBA9OMUBBUXQIAqCCheR0WiU4fv7o+rI\nvpsz7LPPrr1rn/N+rbXX3TXsp763Tn2rnqfqqapUFZIkSZKk/tht1gFIkiRJknZlQ02SJEmSesaG\nmiRJkiT1jA01SZIkSeoZG2qSJEmS1DM21CRJkiSpZ2yoTVGS25Lce5lpC0lOmHZM7bKXjWuZ+e+X\n5DMdxPGnSZ436XK1ea11216mjC8medQy07YmuWKF3x7ZxjDyvjbJLyZ57zixrlLue5I8ftLlanNK\n8tYkr1hm2rOSfHTaMbXLXjauFX7z8SQ/O+E4HpDk45MsU5vbONv2EmW8NMmbVph+WZJfWGH6muqq\nSe6U5MIkW9Ya6yrlPjnJuyZZZl/ZUOuPaj+dmlCD8BXAn0winiF/CrwsyZ4dlC2Npap+uqo+Msq8\n7UHusetc5CuBV62zjKX8b+CPOyhXG8AY2+5UjlkrWaZBuKa4kjwZuKGqPj/J2KrqC8D1SZ40yXK1\nccwi56rqVVX1nFGWkWR7klPXGcNvAudU1c61RbqyqnofcP8kPzPJcvvIhtrms64kT3IYsBU4fSLR\nDKiqq4AvA8dPumxpSgrIuD9O8hDgwKr69ORCalTVZ4ADkzx40mVrQxhn2x17W++R5wHDldFJeTvw\n3I7K1vzbDDn3XLrLr3fSNAQ3tE3bUEvykiTfSHJjki8vntVI48QklyS5Jsm7k9y5nbbYjek5Sb6Z\n5FtJfm+gzGOS/GuS69ppbxj36lCS30hyUZJrk3wgyT0Hpt2W5LlJLm6X9caBabsleU2Sbyf5WpIX\ntPPvnuSVwCOBNya5KcmfDyzyuKXKW8JxwGer6ocDyzw8yT8mubpdZ29oxz+r7VLyZ225lyR5WJJn\nJ7k8yc4kzxwqfwH4pXHWmfqtTzmX5DFJvjAwfFaSTw8MfzTJ8e33H3UFSbJPmu4n1ya5EHjIwG9O\nBe4JvK/Nr98fWOSvJfl6m5cvWyG0J9DkwGCs92/j+06Sq5K8tB2/PcnfJzm1XadfSPKTabq27GyX\nd9xQ+QuYXxtWu62emKar0bVJTklyp4HpT0pyfpsvH097Nnq5bbfdvq5Mcn2Sc5Lcb8y47jOwDX85\nya8OTHtrkr9I8n/b7fiTGeiunORxSf6tjeEv2jhOSHIf4K+Bn29jvnZgkYcsV95QXHsBjwHOGRi3\nW5KXtfujG5Ocm+Tu7bTbkvxWkq+00/4oyY+3+6Drk7xraP9zDvALo+yTNJ/6knPt/v5B7fdntNvq\nfdvhE9J2p8/QVbIkv97+9pokfzAw/vHAS4H/0sZ33sDijkzysTYH/iXJXZaJ6Z7AvYFPDYzbJ00d\n9bL2//jRNN0jF4/1z0pTP/xOkucleUh7bLsubd1ywAKb4XhWVZvuA/wUcDlwaDt8T+De7fcXAp8A\n7gbsSXMgeEc77UjgNpqzZPsAPw1cDfxCO/1BwDE0DeAjgIuAFw4s97bF5SwR04eB32i//zLwlTbO\n3YA/AD4+VM4ZwIHA4W0Mv9hOex5wYRv/wcAHgVuB3YaXM0p5S8T5J8AbBoZ3Bz4PvKZdJ3cCHtZO\nexZwM7CN5izQK4BvAG9o1+1xwI3AvgPl/b80DcGZbyd+JvfpW861ZX0fOKRd5k7gCmC/dtr3gDu3\n814KPLb9fjJN5etg4B7AF4HLB8r90bxD8f9NmxsPAP4DuM8y6+k04PcGhg8ArgR+F9gL2B84pp22\nvf0/HNfm4Q7gMpqD6+7AfwO+NlT+7wL/MOvtwU83n/bv/wXg7sCdgY8Br2inPbDdzh/S7o+f2W6v\ne7bTd9l223HPanNiT+C1wHkD096yWPYScTwL+Gj7fb82t7a1eXo08G3gvu30twLXAP+p3W7/Dnhn\nO+2uwA3AU9rf/g7wQ24/Vm5bXM7Aspctb4k47w98d2jci9t1+JPt8AOAQ9rvtwHvbfPwfsAPgA+1\neX4gzbH3mUPl3QD89Ky3DT/dfHqUczuAF7Xf/w9NHfJ57fDbaI+LNMeNU9vv9wNuAh5Bc3x5DU2d\nbfF493LgbUPLWQAuAX4C2JumTvmqZWL6JeCLQ+P+os2Zw9qcfmi77CPb/PrLdvi4Nr/e2+4H7tau\ny0cNlHVI+5v9Z70ddPnZrFfUbqWpNN0/yZ5VdXlVfa2d9lzgD6vqW1V1M3AS8CvZ9WEAJ1XV96vq\nizSJ83SAqvpcVX26qm6rqq/TJMujx4jveTQb/r9V1W0096scneTwgXlOrqobq+oKmkRZvBH6qcDr\n2vivb387fKl8qUvnw+UdvUxsBwHfHRg+hibhXtyukx9U1ScGpl9aVTuqyarTaJLtj6rq5qo6i+ag\n+xMD899EUwnWxtKrnKuq7wOfaed9MHA+8HGaA9ZDga9U1XVL/PRXgVdW1fVV9Q3g9YzWFeWkNje+\nQHNiY7kHFxxMkwOLngR8q6peW1U/rKrv1q7dIj9SVWdV1a3Ae4C70OTyrcC7ac58Hjgw/3cxvzay\nAt5YVd9st99X0uYKTRehv6mqz1TjbTQVoYcuW1jVW6vq3wfy8meTHLDGmJ7E7ceB26rqfOAfaXJp\n0T9W1bntdvt2bj/+PJGmond6+9s/B64a+N1SuVcrlDdsON8ATgD+oKq+As29ZlU1eLXu1W0eXgRc\nAPxzVV1WVTcC/0xTOR/kMW1j60vOncPtx75H0NT9FocfxcBV4wG/Aryvqj5WTS+p/4+m4bMo3DHH\nCjilqi6pqv+gqdeNlF/tMf3ZNI3GK9uc/mQN9NCiaYj+sK0f3kRz0vaaqvoW8FF2za/Fsjd0fm3K\nhlpVXQL8D5ozCzuTvDPNvVfQtOrf215mvY7mDP0twOATawaf8nY5TeODJEe13S2uTHIDTcIueUl4\nFUcArx+I4Tvt+LsPzDN4sPoezRk+aBpNg/F9Y4nyl7pPbbnyhl1Hc5Z/0eHA19sG5VIGbyD9PkBV\nfXto3OCyDgCuX6Yszame5tw5NPdbPrL9vnigexRD3Q8H3G2JWEYxnF/7LTPfdTRn5hcdDnxtmXmh\nubq46PvANe1JkcVhML82myVzhea48nuLedbm2j0Gpu+i7QJ4ctsF8Aaas//QnN1eiyOAnxta7n/l\n9vwu7nicWNxm78Ydj2FLHdOGLVfesOHjGTQ599U1lL3assy5ja8POfcR4JFJDqW5kvz3wMOTHAEc\n1J4gGbZLflXV97i9vrmSwePZWvLrrjRX4SaVX4tlb+j82pQNNYCqemdVPZImkYrmiWjQJNnjq+rO\nA599q+rKgZ/fc+j7N9vvf0VTyfyJqjqIpsviOOv4cuA3h2LYr6o+OcJvr6Q50Cw6fGj6ep/S9QXg\nqIHhK4B7Jtl9neUuui/N1Q1tMD3MuXNo7k9ZbJgtNtwezdJnH6HJr+FYdvlvjrjs5Qzn1+U0ffyX\nMs6yzK+Nb7lcuZzmavBgnu1fVe9upw9vT8+gebDTL7S5da92/FofZnA5zVPfBpd7QFU9f4Tffoum\nYtssOMng8BIxr9UlbbGHDYy7gl17eYytvbdtL+DfJlGeemvmOdeeDP0e8Ns0+XYTTYPqN2muRC3l\nWwzUEZPsy64nOidxPLvXQO+Ya2i6/k8kv2iOZ5dV1XdXnXOObcqGWnsW/rHtDZ8/oNlwbm0n/zXw\nv9qbIEnyY2kfKjDgD9sbIu9P0594Men2p7kU+700Nzr/1pgh/jXNY+rv18ZwUAZuvl7qv8TtiXwa\n8MIkd0tyMPASdk22ncCPr7L8lXYKHwQelOYmbGhuEr0SODnJvkn2TvKwVcpfyaNpuo9oA+lpzn2C\n5t65hwCfbrsyHQH8HM3ZyaWcBrw0ycFJ7kFzUBw0Sn7B8jn2fnbtuvl/gcOSvLC94fqAJMesUsZK\nHoX5tZEF+O9J7p7kEJoTF4u58ibgeWkewJMk+yX5pSSLZ6iHt939aXL12iT7Af9riWWN4p+Ao5L8\nWpI9289D2nxdrZz3Az+T5JeT7AE8Hzh0YPpO4B7Z9WEdI+dF2+XqgzQnaBa9GXhFkp9o19MD2nW5\nnCzzHZpcPrvtxqaNqU85dw7wAm4/0bgwNDzsH4AnJXl4W6f7I3ZtF1xF031+lNtn7qC9PeASmmMq\nbc+rU4A/S3JYmofc/fxAfXKtHk2zj9jQNmVDjeZemVfR3NB8Jc3l2Je2015P82CNM5PcCPwrzX1Y\ng86h2fg+CPxJVX2wHf/7NF06bqS5V+Zd7NpIGunsRFWdTnO14V3t5e8LgF9coZwaGPcm4EyaMxmf\npTlI3jrQNfH1NPf/XJvkdcuFsFys1bwL40M0N3cvJt6Tac6QXE5zNvKpK5Sz7Dpoz2relw4e/a+Z\n613Otd08PgtcWFW3tKM/QXOG7pplfnYS8HWaLikfoLlJe3AZr6JpVF6X5EUrxLBcfp0H3LDYGGvP\nFB5Hk2NXAhdze6VylPz60XCaR//fVFXnLvN/0/wr4B00x4Cv0jxQ4I8BquqzwHOANwLXttMGn7o7\nvO2+jWZb/ybNQ3P+lTvm1nL59aNp7Zn9xwFPa8u6sl3WXsPzDv2eNg9/FXg1zdn4+wLn0lRmAc6m\neYDHVUmuHvjtyMcdmgf9/PrA8J/RnJA5k+ZBIG+i6a61XDkrrZNn0JyI0sbVl5yD5ji5P7efaBwe\n3qWMqrqQ5uTHO2iurl3Lrt04/7799ztJzh0qY9SYhvPr92nqtJ+h6WY5+ByFUerIg/M8rS1/Q8vt\ntzOsMFNzZebNNE9IKpqbAb9Cc9bgCJqn3jy1modXbFhJjqS5X2SPFe7J6pUkTwD+qqqOnGCZ9wV2\nVNVwZXq95f4pcElVbboDW5KfomlkLLo3zY29f8cmy7NB85hz65Xmkfr/var+84TLfQ/w5qr6wCTL\nnScbPc+SXAqcUFUfmnUsXWi7UF0B/NeqWu4qwTjlfgx4fk3wpddJHkBz7H34pMqcN5uh7rjRc269\n2qtl59E8SXJiL71O86L6Z1TV0yZVZl+N2lDbQdPn9ZS2+8F+NJd3r6mqVyd5Cc2jrE/sNtzZmodK\nY5K9gcfSnN3ZQnNp+xNV9aIVf6jeaCsj36S5qvTbbLI8GzQPOaf5tBHzbCNWGpM8Dvg0zYMEXkzT\nvfneVfWDFX+omdsMdceNmHPql1W7PiY5CHhkVZ0CUFW3VNUNNDc87mhn20HbFW4TWO/NlV0LzZP1\nrgU+R9Mt5H/OMiCt2bE0VxavYPPm2aC+55zmk3k2H36eptvzt2ney/QUG2n9Z91RmoxVr6glOZqm\nD+hFNO/++SzNY7a/UVV3bucJcO3isKTxJTkFOLeq/jLJdeaZNHnmmdQd647SZOwx4jwPAl5QVZ9p\nH0Cxy2Xqqqokd2jxLTVO2giqapwn7q2q7c/9ZJqndQ4v0zzTpmKeSd3rKM+sO0pDxsq1qlrxQ/Mo\n3EsHhh9B8yTBLwGHtuMOA768xG9rtfLH/QDb56nceS17HmOeQtnVYdm/DHxgYPjLs8yzPq5/49kc\nMZlns//bbLblbsb/c1fbdV/rjvP0t5mnmIxnpJhqnN+teo9aVV0FXJFk8SWsx9Lc9/Q+YFs7bhs+\nUl2ahKcD7xwYPgPzTJo080zqkHVHaTJG6foIzROx3t52F/kqzSNWdwdOS3IC7SNWO4lQ2iTaF1we\nS/PelUUnY55JE2OeSVNj3XGOrLfLaZKXTyqWSehbPOMaqaFWzbtFHrLEpGMnG86aLMxZufNadlfl\nznPZnaiqf6d5EfTguGuZbZ6Na2HWAQxZmHUAQxZmHcASFmYdwDTMaZ4tuNwNv+xZLbczPa07jmNh\n1gEsYaGbYsdtqy0AWycXxrot0K944Pb3eq/xV22/yU4kqeroZnBpVvq2XfctHmkS+rZd9y0eaRL6\nuF33MabNoLmi5nNcuhPG2a5XvUdNkiRJkjRdNtQkSZIkqWdsqEmSJElSz4z61EdJPZbs/ZpZx7Ax\n3fL9qlv+cNZRSJKkzceHiUhr1LfturkB+ORZh7EB/QD44x9U/XDvWUeyGfUxz/oUjzQJfdyu+xjT\nZuDDRLo23sNEbKhJa9S37dqda1duAu5iQ21G+phnfYpHmoQ+btd9jGkzsC7RtfEaanZ9lCRJnVvv\nC3XFWBU9SfPLhpokSZoS22rjs40mbTY+9VGSJEmSesaGmiRJkiT1jA01SZIkSeoZG2qSJEmS1DM2\n1CRJkiSpZ3r11MckB8KdToY9d591LPPl1tvg+6+pqktmHYkkSZKk9etVQw3YB/hv8Mo9Zx3IfPnf\n34fvvxOwoTbHkhwMvBm4P80zrJ8NfAV4N3AEcBnw1Kq6flYxSvPOPJMkzYtUdfdOk7W+XT7JFjjw\nUrhhn86C2pAeeAOcf3xVfWTWkWwGa92u11DuDuCcqjolyR7AfsAfANdU1auTvAS4c1WdOByP7ybq\nwk3AXX5Q9cO9Zx3JZtTHPPNlw+vjvmq9MvEXXvdxu+5jTJuB+dm18fLXe9SkHkhyEPDIqjoFoKpu\nqaobgOOBHe1sO4CnzChEae6ZZ5KkeWJDTeqHewHfTvKWJJ9L8qYk+wFbqmpnO89OYMvsQpTmnnkm\nSZobI92jluQy4EbgVuDmqjomySHYp1+alD2ABwEvqKrPJHkdsEvXq6qqpmvCUrYPfN/afqT5kWQr\n3W+468qzJNsHBheqaqGrQKUuTCnPFpd1GdYdpXUZ6R61JJcCD66qawfGvZoJ9+n3HrVxeY/aNHXR\nfz7JocC/VtW92uFHAC8F7g08pqquSnIY8OGqus9wPPYr74L3qM1SH/PM+2bWx33Ves3XPWrTqjtq\nMszPrnV/j9pw4fbplyakqq4CrkhyVDvqWOBC4H3AtnbcNuD0GYQnbQjmmTR11h2ldRj18fwFfDDJ\nrcDfVNWbsE+/NGm/Dbw9yV7AV2keG747cFqSE2i7icwuPGlDMM+k6bDuKK3TqA21h1fVlUl+DDgr\nyZcHJ9qnXxvZtPr0V9XngYcsMenYrpctbRbmmTQ11h21iS20n/UZqaFWVVe2/347yXuBY4CdSQ4d\n6NN/9TK/3b7uKKUZag8QC4vDSV4+s2AkSZoD1h21uW1l13P8J41Vyqr3qCXZN8kB7ff9gMcBFwBn\nYJ9+SZIkDbDuKE3GKFfUtgDvTbI4/9ur6swk52KffkmSJO3KuqM0Aas21KrqUuDoJcZfi336JUmS\nNMC6ozQZa3k8vyRJkiRpCmyoSZIkSVLP2FCTJEmSpJ6xoSZJkiRJPTPqC6/Vf+e0T1fSGlSVK02S\nJEm9Y0NtQ6lZBzBnbKNJkiSpn+z6KEmSJEk9Y0NNkiRJknrGhpokSZIk9YwNNUmSJEnqGRtqkiRJ\nktQzPvVR6okklwE3ArcCN1fVMUkOAd4NHAFcBjy1qq6fWZDSnDPPJEnzwitqUn8UsLWqHlhVx7Tj\nTgTOqqqjgLPbYUnjM88kSXPBhprUL8Mvdzse2NF+3wE8ZbrhSBuSeSZJ6j0balJ/FPDBJOcmeU47\nbktV7Wy/7wS2zCY0acMwzyRJc8F71KT+eHhVXZnkx4Czknx5cGJVVZJa+qfbB75vbT/S/Eiylels\nuGPnWZLtA4MLVbXQXZjS5E0xzyRNQKqWqfdNovCkqmq4i8lK82+BAy+FG/bpLKgN6YE3wPkHNSeK\nNbqwlu3zR79a43Y9jiQvB74LPIfmfpqrkhwGfLiq7jMcj3/7LtwE3OUHVT/ce9aRbEZ9zLOu49no\n3Fet13jHrBVL7OF23ceYNgPzs2vj5a9dH6UeSLJvkgPa7/sBjwMuAM4AtrWzbQNOn02E0vwzzyRJ\n88Suj1I/bAHemwSavHx7VZ2Z5FzgtCQn0D42fHYhSnPPPJMkzY2RGmpJdgfOBb5RVU/2nTPSZFXV\npcDRS4y/Fjh2+hFJG495Jk3PpOuOSfYFDugiVqmvRr2i9kLgIm5PkMV3zrw6yUvaYd87I0mSJJh8\n3fFZsMfrYd+bJxum4LY0t+uqb1ZtqCW5B/BE4JXAi9rRxwOPbr/vABawoSZJkrTpdVd3fNbN8CYf\nODdx1wA/NusgtIRRHibyWuDFwG0D43znjCRJkpZi3VGagBWvqCV5EnB1VZ3XvnvjDlZ+t5PvndH8\n870zkiSNxrqjBM0F44V1l7Ja18eHAccneSKwN3BgklOBnUkOHXjnzNXLFVBV29cdpTRD7QFiYXG4\nffeSJEm6I+uOElvZ9Rz/SWOVsmLXx6p6WVUdXlX3Ap4GfKiqfh3fOSNJkqQh1h2lyVnrC68XL1Of\nDByX5GLgse2wJEmSNMi6ozSmkV94XVXnAOe0333njCRJkpZl3VFan7VeUZMkSZIkdcyGmiRJkiT1\njA01SZIkSeoZG2qSJEmS1DM21CRJkiSpZ2yoSZIkSVLP2FCTJEmSpJ6xoSb1SJLdk5yX5H3t8CFJ\nzkpycZIzkxw86xileWeeSZLmgQ01qV9eCFwEVDt8InBWVR0FnN0OS1of80yS1Hs21KSeSHIP4InA\nm4G0o48HdrTfdwBPmUFo0oZhnkmS5oUNNak/Xgu8GLhtYNyWqtrZft8JbJl6VNLGYp5JkubCHrMO\nQBIkeRJwdVWdl2TrUvNUVSWppabB9oHvW9uPND/a7X5rx8tYV54l2T4wuFBVCxMPUurQNPJM0uTY\nUJP64WHA8UmeCOwNHJjkVGBnkkOr6qokhwFXL/3z7dOKU+pE2+hZWBxO8vIOFrOuPKuq7R3EJE3N\nlPJM0oTY9VHqgap6WVUdXlX3Ap4GfKiqfh04A9jWzrYNOH1WMUrzzjyTJM0TG2pSPy12vToZOC7J\nxcBj22FJk2GeSZJ6y66PUs9U1TnAOe33a4FjZxuRtPGYZ5KkvvOKmiRJkiT1jA01SZIkSeoZG2qS\nJEmS1DMrNtSS7J3kU0nOT3JRkle14w9JclaSi5OcmeTg6YQrSZKkPrP+KE3Gig21qvoP4DFVdTTw\nAOAxSR4BnAicVVVHAWe3w5IkSdrkrD9Kk7Fq18eq+l77dS9gd+A64HhgRzt+B/CUTqKTJEnS3LH+\nKK3fqo/nT7Ib8Dngx4G/qqoLk2ypqp3tLDuBLR3GKEnSzCX5zVnHIM0L64/S+q3aUKuq24CjkxwE\n/EuSxwxNryS19K8hyfaBwYWqWhgzVmkmkmwFts44DEkzt+21s45gfp25G1w56yA0ReupP1p31Pxb\naD/rM/ILr6vqhiT/BDwY2Jnk0Kq6KslhwNUr/G77uqOUZqg9QCwsDid5+cyCkTRDb9131hHMryfc\nCFfuPesoNH3j1B+tO2r+bWXXc/wnjVXKak99vOviE3mS7AMcB5wHnAFsa2fbBpw+1tIlqdduvlOS\n8tPdZ9Z/YUmTZ/1RmozVrqgdBuxo+xnvBpxaVWcnOQ84LckJwGXAU7sNU5JmxbZEdzLrACR1w/qj\nNAErNtSq6gLgQUuMvxY4tqugJEmSNJ+sP0qTserj+SVJkiRJ02VDTZIkSZJ6xoaaJEmSJPWMDTWp\nB5LsneRTSc5PclGSV7XjD0lyVpKLk5y5+BQtSWtnnkmS5okNNakHquo/gMdU1dHAA4DHJHkEcCJw\nVlUdBZzdDksag3kmSZonNtSknqiq77Vf9wJ2B64Djgd2tON3AE+ZQWjShmGeSZLmhQ01qSeS7Jbk\nfGAn8OGquhDYUlU721l2AltmFqC0AZhnkqR5sdoLryVNSVXdBhyd5CDgX5I8Zmh6JVnm7cvbB75v\nbT/S/EiylSlsuOaZNrNp5ZmkybChJvVMVd2Q5J+ABwM7kxxaVVclOQy4eulfbZ9egFIHqmoBWFgc\nTvLyjpdnnmnTmXaeSVofuz5KPZDkrotPmkuyD3AccB5wBrCtnW0bcPpsIpTmn3kmSZonXlGT+uEw\nYEeS3WhOoJxaVWcnOQ84LckJwGXAU2cYozTvzDNJ0tywoSb1QFVdADxoifHXAsdOPyJp4zHPJEnz\nxK6PkiRJktQzNtQkSZIkqWdsqEmSJElSz9hQkyRJkqSesaEmSZIkST1jQ02SJEmSesaGmiRJkiT1\nzKoNtSSHJ/lwkguTfDHJ77TjD0lyVpKLk5yZ5ODuw5UkSVKfWXeUJmOUK2o3A79bVfcHHgo8P8l9\ngROBs6rqKODsdliSJEmbm3VHaQJWbahV1VVVdX77/bvAl4C7A8cDO9rZdgBP6SpISZIkzQfrjtJk\nrOketSRHAg8EPgVsqaqd7aSdwJaJRiZJkqS5Zt1RGt8eo86YZH/gH4AXVtVNSX40raoqSS3zu+0D\ngwtVtTBeqNJsJNkKbJ1xGJIkzRXrjtq8FtrP+ozUUEuyJ02inVpVp7ejdyY5tKquSnIYcPVSv62q\n7euOUpqh9gCxsDic5OUzC0aSpDlg3VGb21Z2Pcd/0liljPLUxwB/C1xUVa8bmHQGsK39vg04ffi3\nkiRJ2lysO0qTMcoVtYcDvwZ8Icl57biXAicDpyU5AbgMeGonEUqSJGmeWHeUJmDVhlpVfYzlr7wd\nO9lwpM0pyeHA24D/Byjg/1TVnyc5BHg3cATtQa2qrp9ZoNIcM8+k6bDuKE3Gmp76KKkzvnNG6p55\nJkmaGzbUpB7wnTNS98wzSdI8saEm9YzvnJG6Z55Jkvpu5PeoSereuO+cge0D37fia980b6b5vkLz\nTJuV7wWV5osNNakn1vPOmV0rkNL8mdb7Cs0zbWa+F1SaL3Z9lHrAd85I3TPPJEnzxCtqUj/4zhmp\ne+aZJGlu2FCTesB3zkjdM88kSfPEro+SJEmS1DM21CRJkiSpZ2yoSZIkSVLP2FCTJEmSpJ6xoSZJ\nkiRJPWNDTZIkSZJ6xoaaJEmSJPWMDTVJkiRJ6hkbapIkSZLUMzbUJEmSJKlnbKhJkiRJUs+s2lBL\nckqSnUkuGBh3SJKzklyc5MwkB3cbpiRJkuaBdUdpMka5ovYW4PFD404Ezqqqo4Cz22FJkiTJuqM0\nAas21KqbU9zXAAALrElEQVTqo8B1Q6OPB3a033cAT5lwXJIkSZpD1h2lyRj3HrUtVbWz/b4T2DKh\neCRJkrTxWHeU1miP9RZQVZWklpueZPvA4EJVLax3mdI0JdkKbO14GacAvwRcXVU/0447BHg3cARw\nGfDUqrq+yzikjc5ck2bPuqM2voX2sz7jNtR2Jjm0qq5Kchhw9XIzVtX2MZch9UJ7gFhYHE7y8g4W\n8xbgDcDbBsYt9ud/dZKXtMP26ZfWx1yTZsO6ozaRrex6jv+ksUoZt+vjGcC29vs24PQxy5GE/fml\naTHXpJmx7iit0SiP538n8Angp5JckeTZwMnAcUkuBh7bDkuaLPvzS9NhrkkTZN1RmoxVuz5W1dOX\nmXTshGORtIzV+vPD9oHvW+n4ljpp4qZxL+goVs617QPft9KDcKU1mVaeWXeUJmPdDxOR1JmR+/Pv\nWoGU5s+U7gVdzoi5tn2KIUmTN+M8k7RG496jJql79ueXpsNckyT1jg01qQfszy9Nh7kmSZoXdn2U\nesD+/NJ0mGuSpHnhFTVJkiRJ6hkbapIkSZLUMzbUJEmSJKlnbKhJkiRJUs/YUJMkSZKknrGhJkmS\nJEk9Y0NNkiRJknrGhpokSZIk9YwNNUmSJEnqGRtqkiRJktQzNtQkSZIkqWdsqEmSJElSz9hQkyRJ\nkqSesaEmSZIkST1jQ02SJEmSemZdDbUkj0/y5SRfSfKSSQU1moU5K7frsruyYNkzNts8G9fCrAMY\nsjDrAIYszDqAJSzMOoCZ6neeLbjcDb/sWS13+vqda0tZmHUAS1iYdQBDFmYdwJCFWQcwMWM31JLs\nDrwReDxwP+DpSe47qcBWtzBn5XZddlcWLHuGZp9n41qYdQBDFmYdwJCFWQewhIVZBzAz/c+zBZe7\n4Zc9q+VOV/9zbSkLsw5gCQuzDmDIwqwDGLIw6wAmZj1X1I4BLqmqy6rqZuBdwC9PJixJLfNM6p55\nJk2HuSatwR7r+O3dgSsGhr8B/Nz6wgH47l7w2BtWn+9re8NH/mP9y5tWuV2WffHeky9TPTFino2S\nM9PUZR6NY5x4bglwYCfhqG96nmezyqdJLve8O02mHM25ddYd35fp52Hfjmcw+Zh+uBtwwOTK06Ss\np6FWo8yUZKT5bncb8OGDRpv36x3t+Lsqt+uy01G5J3VU7uzLXvv2OXUjxjdqzkxTl9v6OMaNZx7z\nalzTj6knOTgHeTarfJr0ckfNp1nmx6yWvSGOWatZZ91xJ7BzBien+3Y8g25iWs/xrm/HtL7FM571\nNNS+CRw+MHw4zZmRH6mqrmo40mZhnkndM8+k6TDXpDVYzz1q5wI/meTIJHsB/wU4YzJhSWqZZ1L3\nzDNpOsw1aQ3GvqJWVbckeQHwL8DuwN9W1ZcmFpkk80yaAvNMmg5zTVqbVM17d2dJkiRJ2ljW9cLr\nRUl2T3JekvctM/3P2xcbfj7JAydVdpKtSW5op5+X5A/XUO5lSb7Q/u7Tk4x7tbLHjTvJwUnek+RL\nSS5K8tAJxrxi2euI+acGfnNeW8bvTCLuUcpeR9wvTXJhkguSvCPJHW7aXc92vVZJDk/y4TamLy61\nDqcZ0yjxrCc/x4xp7ySfSnJ+uw2/apn5prWOVo1n2uuoXWZn++suYprFOhpa/ilJdia5YMrLHSnn\nO1r2SLnU4fJX3EY7WuaqdYIOl73qsb2DZY50bO5w+au+9Hra+6LVYprBMW3Vfc+U6yErxjOD9dOr\netGoMa15PVXVuj/Ai4C3A2csMe2JwPvb7z8HfHKCZW9davyI5V4KHLLC9LHjHqHsseIGdgC/0X7f\nAzhogjGvVvbY63qgjN2AK4HDJ7mNrFL2muMGjgS+BtypHX43sG3SMa8xpkOBo9vv+wP/Btx3VjGN\nGM+6t5kx4tq3/XcP4JPAI2b8d1stnlmso8721x3FNPV1NLT8RwIPBC6Y8nJXzbGOl7/itjur7aHD\nZa543O542Ssef6ew/CWPnx0ub3fgkvZYuydw/iyPZ2uIaar7otX2PTNYR6vFM+3106t60RpiWtN6\nWvcVtST3aFfEm1n6uZ7H0+yEqKpPAQcn2TKhsllh/EiLWGHa2HGPGNea4k5yEPDIqjqljemWqhp+\nl8hYMY9Y9ppjXsKxwFer6oqh8etd1yuVDWuP+0bgZmDfJHsA+9I8qWrQJGIeWVVdVVXnt9+/C3wJ\nuNusYhoxHuju2fbLxfW99uteNAfea4dmmfbfbbV4YIrrqMv9dYcxscL4zlXVR4HrZrDcUXOsq+WP\nsu1O3IjbQ2eLn/Ly1nL87dJKx88ujPLS62nvi0Z9EffUtpER9j3TPp6Nsi+c5vrpVb1oDTHBGtbT\nJLo+vhZ4Mc0L0Jay1MsN7zGhsgt4WHs58/1J7jdiuYu//WCSc5M8Z4np64l7tbLHiftewLeTvCXJ\n55K8Kcm+E4p5lLLXs64XPQ14xxLj17OuVyt7zXFX1bXAa4DLgW8B11fVBzuIeSxJjqQ5q/WpPsS0\nQjyT2GbWGstuSc6nednOh6vqoqFZprqORohn2uuoy/11VzFNfTvqmxVyrMtlrrbtdmW17aErqx23\nuzLK8bdryx0/u7LUfubuI8zT5b5olJj6ti+aWT1kGTNbP32rF60S05rW07oaakmeBFxdVeexcutw\neNqqTzAZsezP0Vyq/1ngDcDpq0f9Iw+vqgcCTwCen+SRS4UxNDzqk1dWK3ucuPcAHgT8ZVU9CPh3\n4MQJxTxK2etZ16R5DO+Tgb9fbpah4ZGfcrNK2WuOO8mPA/+DpgvE3YD9kzxjkjGPK8n+wHuAF7Zn\na2Ya0yrxrGubGUdV3VZVR9PsiB+VZOsSs01tHY0Qz9TWUZf7645jmvp21Ccj5HwnRsyliVrDNtqF\nUeoEXRj12N6JEY7NXRh1nzLN49koZfdxXzT1esgKZrJ++lYvGiGmNa2n9V5RexhwfJJLgXcCj03y\ntqF5hl9ueA/u2I1srLKr6qbF7hlV9c/AnkkOGSXwqrqy/ffbwHtpLntPIu5Vyx4z7m8A36iqz7TD\n76HZuU8i5lXLXs+6bj0B+Gy7ToaNva5XK3vMuP8T8Imq+k5V3QL8I832OMmY1yzJnsA/AH9XVUsl\n9lRjWi2eCWwzY2u7Dv0Tzd9y0NT/bivFM+V11OX+urOYZrkdzdoIOd+5FXKpC6Nso50YoU7QlVGO\n7V1a6djclVVfer3EPF3vi0Z5EXff9kUzOZ4tZxbrp2/1olFiWut6WldDrapeVlWHV9W9aC6df6iq\nnjk02xnAM9vgH0rTjWznJMpOsiVJ2u/H0LxuYNV+9En2TXJA+30/4HHA8FNsxop7lLLHibuqrgKu\nSHJUO+pY4MJJxDxK2eOu6wFPpznwLmWsuEcpe8y4vww8NMk+7W+PBYa7/aw35jVp4/hb4KKqet0y\ns00tplHimcA2s9aY7prk4Pb7PsBxwHlDs01zHa0azzTXUZf76y5jmvZ21Bcj5nxXyx4llyZuxG10\n4kasE3RixGN7l1Y6NndllJdeT3VfNEpMPdwXTXsdrWgGx/xe1YtGjWmt62nsF14vo9oFPxegqv6m\nqt6f5IlJLqG5pP/sSZUN/ArwW0luAb5Hs2MfxRbgve162gN4e1WdOaG4Vy17HXH/NvD2difyVeA3\nJriuVyx7HTEvHviOBZ4zMG4ica9W9jhxV9Xn27O459LcJ/E54E0dbdejejjwa8AXkixWmF4G3HNG\nMa0aD+vYZsZ0GLAjyW40J6FOraqzZ/h3WzUepr+OBnW5v55YTMx2HZHkncCjgbskuQL4n1X1liks\neqkce2lVfWAKy15y253CcodNqxvXksftKS0b7nj8nUreLXX8nIZa5qXXs9wXjRITU94XDex77tru\ne15O80TKmayj1eJh+vvqvtWLRoqJNa4nX3gtSZIkST0zkRdeS5IkSZImx4aaJEmSJPWMDTVJkiRJ\n6hkbapIkSZLUMzbUJEmSJKlnbKhJkiRJUs/YUJMkSZKknvn/AbGDiJ7+R+9YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15204d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Iterate over the features, creating a subplot with a histogram for each one.\n",
    "for feature in range(train_data.shape[1]):\n",
    "    plt.subplot(1, 4, feature+1)\n",
    "    plt.hist(train_data[:,feature], 2)\n",
    "    plt.title(iris.feature_names[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function that applies a threshold to turn real valued iris features into 0/1 features.\n",
    "# 0 will mean \"short\" and 1 will mean \"long\".\n",
    "def binarize_iris(data, thresholds=[6.0, 3.0, 2.5, 1.0]):\n",
    "    # Initialize a new feature array with the same shape as the original data.\n",
    "    binarized_data = np.zeros(data.shape)\n",
    "\n",
    "    # Apply a threshold  to each feature.\n",
    "    for feature in range(data.shape[1]):\n",
    "        binarized_data[:,feature] = data[:,feature] > thresholds[feature]\n",
    "    return binarized_data\n",
    "\n",
    "# Create new binarized training and test data\n",
    "binarized_train_data = binarize_iris(train_data)\n",
    "binarized_test_data = binarize_iris(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Naive Bayes assumes conditional independence of features. With $Y$ the set of labels and $X$ the set of features ($y$ is a specific label and $x$ is a specific feature), Naive Bayes gives the probability of a label $y$ given input features $X$ as:\n",
    "\n",
    "$ \\displaystyle P(y|X) \\approx \n",
    "  \\frac { P(y) \\prod_{x \\in X} P(x|y) }\n",
    "        { \\sum_{y \\in Y} P(y) \\prod_{x \\in X} P(x|y) }\n",
    "$\n",
    "\n",
    "Let's estimate some of these probabilities using maximum likelihood, which is just a matter of counting and normalizing. \n",
    "\n",
    "We'll start with the ***prior* probability** of the label: $P(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n",
      "[31, 33, 36]\n",
      "100\n",
      "         setosa : 0.31\n",
      "     versicolor : 0.33\n",
      "      virginica : 0.36\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for all labels to zero.\n",
    "label_counts = [0 for i in iris.target_names]\n",
    "'''  print label_counts  '''\n",
    "print iris.target_names\n",
    "\n",
    "# Calculate the counts of labels in the training data set by iterating over labels.\n",
    "for label in train_labels:\n",
    "    label_counts[label] += 1\n",
    "print label_counts\n",
    "\n",
    "# Normalize counts to get the estimates of the probabilities of each label.\n",
    "total = sum(label_counts)\n",
    "print total\n",
    "label_probs = [1.0 * count / total for count in label_counts]\n",
    "for (prob, name) in zip(label_probs, iris.target_names):\n",
    "    print '%15s : %.2f' %(name, prob)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of what zip() function does (from https://docs.python.org/2/library/functions.html#zip):             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "zipped = zip(x, y)\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have estimated the prior probabilities of each label, $P(y)$\n",
    "\n",
    "Next, let's estimate $P(X|Y)$, that is, the ***probability of each feature given each label***: if I am a flower labeled $y (e.g., setosa)$, what is the probability that my measurements (features) will be $x$?  \n",
    "\n",
    "Remember that we can get the conditional probability from the joint distribution:\n",
    "\n",
    "$\\displaystyle P(X|Y) = \\frac{ P(X,Y) } { P(Y) } \\approx \\frac{ \\textrm{Count}(X,Y) } { \\textrm{Count}(Y) }$\n",
    "\n",
    "Let's think carefully about the size of the count matrix we need to build. There are 3 labels $y_1$, $y_2$, and $y_3$ ($setosa$, $versicolor$, and $virginica$) and 4 features $x_0$, $x_1$, $x_2$, and $x_3$ ($petalLength$, $petalWidth$, $sepalLength$, and $sepalWidth$). Each feature has 2 possible values, 0 or 1. So there are actually $3 \\times 4 \\times 2=24$ probabilities we need to estimate: \n",
    "\n",
    "$P(x_0=0, Y=y_0)$\n",
    "\n",
    "$P(x_0=1, Y=y_0)$\n",
    "\n",
    "$P(x_1=0, Y=y_0)$\n",
    "\n",
    "$P(x_1=1, Y=y_0)$\n",
    "\n",
    "...\n",
    "\n",
    "However, we already estimated (above) the probability of each label, $P(y)$. And, we know that each feature value is either 0 or 1 **(the advantage of having converted the problem to binary)**. So, for example,\n",
    "\n",
    "$P(x_0=0, Y=\\textrm{setosa}) + P(x_0=1, Y=\\textrm{setosa}) = P(Y=\\textrm{setosa}) \\approx 0.31$.\n",
    "\n",
    "As a result, we can just estimate probabilities for one of the feature values, say, $x_i = 0$. This requires a $4 \\times 3$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L, 4L)\n",
      "Feature = 0 and label:\n",
      "[[ 31.  20.   7.]\n",
      " [  6.  27.  25.]\n",
      " [ 31.   0.   0.]\n",
      " [ 31.   3.   0.]]\n",
      "\n",
      "Feature = 1 and label:\n",
      "[[  0.  13.  29.]\n",
      " [ 25.   6.  11.]\n",
      " [  0.  33.  36.]\n",
      " [  0.  30.  36.]]\n",
      "\n",
      "Total count: 400.0\n",
      "Label probabilities: [ 0.31  0.33  0.36]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a matrix for joint counts of feature=0 and label.\n",
    "feature0_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "'''print feature0_and_label_counts'''\n",
    "\n",
    "# Just to check our work, let's also keep track of joint counts of feature=1 and label.\n",
    "feature1_and_label_counts = np.zeros([len(iris.feature_names), len(iris.target_names)])\n",
    "'''print feature1_and_label_counts'''\n",
    "\n",
    "print binarized_train_data.shape\n",
    "'''binarized_train_data.shape[1] corresponds to the rows of data'''\n",
    "for i in range(binarized_train_data.shape[0]):   \n",
    "    # Pick up one training example at a time: a label and a feature vector.\n",
    "    label = train_labels[i]\n",
    "    features = binarized_train_data[i]\n",
    "    \n",
    "    # Update the count matrices.\n",
    "    for feature_index, feature_value in enumerate(features):\n",
    "        feature0_and_label_counts[feature_index][label] += (feature_value == 0)\n",
    "        feature1_and_label_counts[feature_index][label] += (feature_value == 1)\n",
    "\n",
    "# Let's look at the counts.\n",
    "print 'Feature = 0 and label:\\n', feature0_and_label_counts\n",
    "print '\\nFeature = 1 and label:\\n', feature1_and_label_counts\n",
    "\n",
    "# As a sanity check, what should the total sum of all counts be?\n",
    "# We have 100 training examples, each with 4 features. So we should have counted 400 things.\n",
    "total_sum = feature0_and_label_counts.sum() + feature1_and_label_counts.sum()\n",
    "print '\\nTotal count:', total_sum\n",
    "\n",
    "# As another sanity check, the label probabilities should be equal to the normalized feature counts for each label.\n",
    "print 'Label probabilities:', (feature0_and_label_counts.sum(0) + feature1_and_label_counts.sum(0)) / total_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to normalize the joint counts to get probabilities: ***P(feature|label) = P(feature, label) / P(label)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated values of P(feature=0|label):\n",
      "[[ 1.          0.60606061  0.19444444]\n",
      " [ 0.19354839  0.81818182  0.69444444]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 1.          0.09090909  0.        ]]\n",
      "\n",
      "Check that P(feature=0|label) + P(feature=1|label) = 1\n",
      "[[ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize new matrices to hold conditional probabilities.\n",
    "feature0_given_label = np.zeros(feature0_and_label_counts.shape)\n",
    "feature1_given_label = np.zeros(feature1_and_label_counts.shape)\n",
    "\n",
    "# P(feature|label) = P(feature, label) / P(label) =~ count(feature, label) / count(label).\n",
    "# Note that we could do this normalization more efficiently with array operations, but for the sake of clarity,\n",
    "# let's iterate over each label and each feature.\n",
    "for label in range(feature0_and_label_counts.shape[1]):\n",
    "    for feature in range(feature0_and_label_counts.shape[0]):\n",
    "        feature0_given_label[feature,label] = feature0_and_label_counts[feature,label] / label_counts[label]\n",
    "        feature1_given_label[feature,label] = feature1_and_label_counts[feature,label] / label_counts[label]\n",
    "\n",
    "# Here's our estimated conditional probability table.\n",
    "print 'Estimated values of P(feature=0|label):\\n', feature0_given_label\n",
    "\n",
    "# As a sanity check, which probabilities should sum to 1?\n",
    "print '\\nCheck that P(feature=0|label) + P(feature=1|label) = 1\\n',feature0_given_label + feature1_given_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces, let's try making a **prediction** for the first test example. It looks like this is a setosa (label 0) example with all small measurements -- all the feature values are 0.\n",
    "\n",
    "We **start by *assuming the prior distribution*** , which has a slight preference for virginica, followed by versicolor. Of course, these estimates come from our training data, which might not be a representative sample. In practice, we ***may*** prefer to use a uniform prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [ 0.  0.  0.  0.]\n",
      "Label: 0\n",
      "Prior: [0.31, 0.33, 0.36]\n"
     ]
    }
   ],
   "source": [
    "# What does the feature vector look like? And what's the true label?\n",
    "index = 0\n",
    "print 'Feature vector:', binarized_test_data[index]\n",
    "print 'Label:', test_labels[index]\n",
    "\n",
    "# Start with the prior distribution over labels.\n",
    "predictions = label_probs[:]\n",
    "print 'Prior:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of each feature as an additional piece of evidence. After observing the first feature, we update our belief by multiplying our initial probabilities by the probability of the observation, conditional on each possible label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After observing sepal length: [ 0.53448276  0.34482759  0.12068966]\n"
     ]
    }
   ],
   "source": [
    "# Let's include the first feature. We use feature0_given_label since the feature value is 0.\n",
    "predictions *= feature0_given_label[0]\n",
    "\n",
    "# We could wait until we've multiplied by all the feature probabilities, but there's no harm in normalizing after each update.\n",
    "predictions /= predictions.sum()\n",
    "print 'After observing sepal length:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after observing a short sepal, our updated belief prefers setosa. Let's include the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After observing all features: [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Include the second feature.\n",
    "predictions *= feature0_given_label[1]\n",
    "predictions *= feature0_given_label[2]\n",
    "predictions *= feature0_given_label[3]\n",
    "# print feature0_given_label\n",
    "# print feature1_given_label\n",
    "\n",
    "# We could wait until we've multiplied by all the feature probabilities, but there's no harm in normalizing after each update.\n",
    "predictions /= predictions.sum()\n",
    "print 'After observing all features:', predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?\n",
    "\n",
    "Well, it looks like Naive Bayes came up with the right answer. But it seems overconfident!\n",
    "\n",
    "Let's look again at our conditional probability estimates for the features, **feature0_given_label** and **feature1_given_label**. Notice that there are a bunch of ***zero probabilities***. This is bad because as soon as we **multiply anything by zero**, we're guaranteed that our final estimate will be zero. This is an overly harsh penalty for an observation that simply never occurred in our training data. Surely there's some possibility, even if very small, that there could exist a setosa with a long sepal.\n",
    "\n",
    "This is where ***smoothing*** comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood estimate is only optimal in the case where we have infinite training data. When we have less than that, we need to temper maximum likelihood by reserving some ***small probability for unseen events***. The simplest way to do this is with Laplace smoothing (http://en.wikipedia.org/wiki/Additive_smoothing) -- rather than starting with a count of 0 for each joint (feature, label) observation, we start with a count of **$\\alpha$**.  Note that the $\\alpha$ is applied during the training step, to produce the **label_counts** and to initialize the **feature0_and_label_counts**, which is later used to compute the **feature0_and_label_counts**.\n",
    "\n",
    "Now we have covered everyting we need to package training and inference into a class.  This **NaiveBayes** class below has been modeled after sklearn's BernoulliNB (details here:  http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha     # additive (Laplace) smoothing parameter\n",
    "        self.priors = None     # estimated by fit()\n",
    "        self.probs = None      # estimated by fit()\n",
    "        self.num_labels = 0    # set by fit()\n",
    "        self.num_features = 0  # set by fit()\n",
    "        \n",
    "    def fit(self, train_data, train_labels):\n",
    "        # Store number of labels, number of features, and number training examples.\n",
    "        self.num_labels = len(np.unique(train_labels))\n",
    "        self.num_features = train_data.shape[1]\n",
    "        self.num_examples = train_data.shape[0]\n",
    "        \n",
    "        # Initialize an array of (feature=1, label) counts to alpha.\n",
    "        feature0_and_label_counts = np.ones([self.num_features, self.num_labels]) * self.alpha\n",
    "        '''We do not care for feature1_and_label_counts (it is 1-feature0_and_label_counts), \n",
    "        but we would use the same initialization for it'''\n",
    "      \n",
    "        # Initialize an array of label counts. Each label gets a smoothed count of 2*alpha because\n",
    "        # each feature value (0 and 1) gets an extra count of alpha (see details in the Wkipedia page on Additive Smoothing).\n",
    "        label_counts = np.ones(self.num_labels) * self.alpha * 2\n",
    "\n",
    "        # Count features with value == 1.\n",
    "        for i in range(self.num_examples):\n",
    "            label = train_labels[i]\n",
    "            label_counts[label] += 1\n",
    "            for feature_index, feature_value in enumerate(train_data[i]):\n",
    "                feature0_and_label_counts[feature_index][label] += (feature_value == 1)\n",
    "\n",
    "        # Normalize to get probabilities P(feature=1|label).\n",
    "        self.probs = feature0_and_label_counts / label_counts\n",
    "        \n",
    "        # Normalize label counts to get prior probabilities P(label).\n",
    "        self.priors = label_counts / label_counts.sum()\n",
    "\n",
    "    # Make predictions for each test example and return results.\n",
    "    ''' Nothing new here: same predict() method as we used in NearestNeighbors class'''\n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return np.array(results)\n",
    "    \n",
    "    # Private function for making a single prediction.\n",
    "    def _predict_item(self, item):\n",
    "        # Make a copy of the prior probabilities.\n",
    "        predictions = self.priors.copy()\n",
    "        \n",
    "        # Multiply by each conditional feature probability.\n",
    "        for (index, value) in enumerate(item):\n",
    "            feature_probs = self.probs[index]\n",
    "            if not value:  ##  same as \"if value != 1\" \n",
    "                feature_probs = 1 - feature_probs\n",
    "            predictions *= feature_probs\n",
    "\n",
    "        # Normalize to the [0,1] range and return the label that gives the largest probability.\n",
    "        predictions /= predictions.sum()\n",
    "        #print item, predictions\n",
    "        return predictions.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy method **argmax** called with ***axis=None*** (default) returns the indices of the array being passed where the value reaches its maximum.  For more details, see here: http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare our implementation with the sklearn implementation. Do the predictions agree? What about the estimated parameters? Try changing alpha from 0 to 1.\n",
    "\n",
    "Note: I think there might be a bug in the sklearn code. What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the model on binarized_train_data\n",
      "Using our NB classifier\n",
      "With alpha = 0.00\n",
      "[OUR implementation] total:  50  correct:  41  accuracy: 0.82\n",
      "Using sklearn's NB classifier\n",
      "sklearn accuracy: 0.52\n",
      "\n",
      "Our feature probabilities\n",
      "[[ 0.          0.39393939  0.80555556]\n",
      " [ 0.80645161  0.18181818  0.30555556]\n",
      " [ 0.          1.          1.        ]\n",
      " [ 0.          0.90909091  1.        ]]\n",
      "\n",
      "sklearn feature probabilities\n",
      "[[ 0.          0.39393939  0.80555556]\n",
      " [ 0.80645161  0.18181818  0.30555556]\n",
      " [ 0.          1.          1.        ]\n",
      " [ 0.          0.90909091  1.        ]]\n",
      "\n",
      "Our prior probabilities\n",
      "[ 0.31  0.33  0.36]\n",
      "\n",
      "sklearn prior probabilities\n",
      "[ 0.31  0.33  0.36]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0\n",
    "nb = NaiveBayes(alpha=alpha)\n",
    "nb.fit(binarized_train_data, train_labels)\n",
    "print \"Trained the model on binarized_train_data\"\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "print \"Using our NB classifier\"\n",
    "preds = nb.predict(binarized_test_data)\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, test_labels):\n",
    "    if pred == label: correct += 1\n",
    "    total += 1\n",
    "print 'With alpha = %.2f' %alpha\n",
    "print '[OUR implementation] total: %3d  correct: %3d  accuracy: %3.2f' %(total, correct, 1.0*correct/total)\n",
    "\n",
    "# Compare to sklearn's implementation.\n",
    "print \"Using sklearn's NB classifier\"\n",
    "clf = BernoulliNB(alpha=alpha)\n",
    "clf.fit(binarized_train_data, train_labels)\n",
    "print 'sklearn accuracy: %3.2f' %clf.score(binarized_test_data, test_labels)\n",
    "\n",
    "print '\\nOur feature probabilities\\n', nb.probs\n",
    "print '\\nsklearn feature probabilities\\n', np.exp(clf.feature_log_prob_).T\n",
    "\n",
    "print '\\nOur prior probabilities\\n', nb.priors\n",
    "print '\\nsklearn prior probabilities\\n', np.exp(clf.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
